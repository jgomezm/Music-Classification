{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import class_weight\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "Category = \"Country\"\n",
    "countriesOfInterest = [\"HK\", \"JP\", 'ZA', 'TN', 'TR', 'GB', 'MX', 'US', 'CO', 'EC', 'AU', 'NZ']\n",
    "countriesOfInterest = [\"HK\", \"JP\", 'ZA', 'TN', 'TR', 'GB', 'MX', 'US', 'CO', 'EC', 'AU']\n",
    "\n",
    "w_length = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import track data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HK has 92 playlists\n",
      "JP has 66 playlists\n",
      "ZA has 119 playlists\n",
      "TN has 107 playlists\n",
      "TR has 82 playlists\n"
     ]
    }
   ],
   "source": [
    "allTracks = pd.DataFrame()\n",
    "for country in countriesOfInterest:\n",
    "    ls = glob.glob(\"Raw Track Data\\\\\" + country + \"*.csv\")\n",
    "    print(country, \"has\", str(len(ls)), \"playlists\")\n",
    "    for file in ls:\n",
    "        if random.random() < 1:\n",
    "            new = pd.read_csv(file)\n",
    "            new[\"Country\"] = file[15:17]\n",
    "            new[\"Year\"] = file[18:22]\n",
    "            new[\"Playlist\"] = file[23:-4]\n",
    "            allTracks = allTracks.append(new)\n",
    "allTracks = allTracks.drop([\"confidence\", \"loudness_start\", \"loudness_max_time\"], axis = 1)\n",
    "del new\n",
    "print(\"Unique tracks:\\t\", len(pd.unique(allTracks.track_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove tracks in multiple countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold = allTracks.groupby(\"track_id\").nunique()\n",
    "keep = hold[hold.Country==1].index\n",
    "allTracks = allTracks[allTracks.track_id.isin(keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allTracks.shape)\n",
    "allTracks = allTracks.drop_duplicates([\"track_id\", \"start\"])\n",
    "print(allTracks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(hold.Country)/hold.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit one-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = pd.unique(allTracks[Category])\n",
    "print(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit(cats.reshape(-1, 1))\n",
    "pickle.dump( enc, open( \"pickle\\\\enc.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate training, testing, and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UniqueTracks = pd.unique(allTracks.track_id)\n",
    "testTracks = np.random.choice(UniqueTracks, int(len(UniqueTracks) * .1), replace = False)\n",
    "test = allTracks.loc[allTracks.track_id.isin(testTracks)]\n",
    "allTracks = allTracks.loc[~allTracks.track_id.isin(testTracks)]\n",
    "UniqueTracks = pd.unique(allTracks.track_id)\n",
    "testTracks = np.random.choice(UniqueTracks, int(len(UniqueTracks) * .2), replace = False)\n",
    "val = allTracks.loc[allTracks.track_id.isin(testTracks)]\n",
    "train = allTracks = allTracks.loc[~allTracks.track_id.isin(testTracks)]\n",
    "del UniqueTracks, testTracks, allTracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, cat):\n",
    "    X = X.reset_index()\n",
    "    new_pos = list(X.track_id.index[X.track_id.shift(1) != X.track_id]) # indices where the song changes\n",
    "    new_pos.append(max(X.track_id.index) + 1) # add a new index to know where the last song ends\n",
    "    split_pos = []\n",
    "    for i in range(len(new_pos)-1):\n",
    "        split_pos = split_pos + list(range(new_pos[i], new_pos[i+1], w_length))\n",
    "    split_pos = split_pos[1:]\n",
    "    us_train = np.split(X.iloc[:,:27].to_numpy(), split_pos)\n",
    "    labs = np.split(X[Category].to_numpy(), split_pos)\n",
    "    # drop the short sequences\n",
    "    short_seqs = []\n",
    "    temp = [] \n",
    "    labels = []\n",
    "    for i, value in enumerate(us_train):\n",
    "        if value.shape[0] == w_length:\n",
    "            temp.append(value)\n",
    "            labels.append(labs[i][0])\n",
    "    us_train = temp\n",
    "    return np.stack(us_train), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_labels= split(test, Category)\n",
    "del(test)\n",
    "val_x, val_labels = split(val, Category)\n",
    "del val\n",
    "train_x, train_labels = split(train, Category)\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( test_x, open( \"pickle\\\\test_x.p\", \"wb\" ) )\n",
    "pickle.dump( val_x, open( \"pickle\\\\val_x.p\", \"wb\" ) )\n",
    "pickle.dump( train_x, open( \"pickle\\\\train_x.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate class weights for unbalanced calsses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_labels),\n",
    "                                                 list(train_labels))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( class_weights, open( \"pickle\\\\class_weights.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = enc.transform(np.array(test_labels).reshape(-1, 1)).toarray()\n",
    "val_labels = enc.transform(np.array(val_labels).reshape(-1, 1)).toarray()\n",
    "train_labels = enc.transform(np.array(train_labels).reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( test_labels, open( \"pickle\\\\test_labels.p\", \"wb\" ) )\n",
    "pickle.dump( val_labels, open( \"pickle\\\\val_labels.p\", \"wb\" ) )\n",
    "pickle.dump( train_labels, open( \"pickle\\\\train_labels.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
