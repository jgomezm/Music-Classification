{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, LSTM, Flatten, BatchNormalization, Dropout\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>,\n",
       "              handle_unknown='error', sparse=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countriesOfInterest = [\"HK\", \"J P\", 'ZA', 'TN', 'TR', 'GB', 'MX', 'US', 'CO', 'EC', 'AU', 'NZ']\n",
    "countriesOfInterest = [\"ZA\", \"EG\", \"TW\", \"JP\", \"DK\", \"FI\", \"US\", \"CA\", \"AU\", \"NZ\", \"BE\", \"CO\"]\n",
    "train_n = 500\n",
    "val_n = 20\n",
    "Category = \"Country\"\n",
    "w_length = 300\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(np.array(countriesOfInterest).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, cat):\n",
    "    X = X.reset_index()\n",
    "    new_pos = list(X.track_id.index[X.track_id.shift(1) != X.track_id]) # indices where the song changes\n",
    "    new_pos.append(max(X.track_id.index) + 1) # add a new index to know where the last song ends\n",
    "    split_pos = []\n",
    "    for i in range(len(new_pos)-1):\n",
    "        split_pos = split_pos + list(range(new_pos[i], new_pos[i+1], w_length))\n",
    "    split_pos = split_pos[1:]\n",
    "    us_train = np.split(X.iloc[:,:24].to_numpy(), split_pos)\n",
    "    labs = np.split(X[Category].to_numpy(), split_pos)\n",
    "    # drop the short sequences\n",
    "    short_seqs = []\n",
    "    temp = [] \n",
    "    labels = []\n",
    "    for i, value in enumerate(us_train):\n",
    "        if value.shape[0] == w_length:\n",
    "            temp.append(value)\n",
    "            labels.append(labs[i][0])\n",
    "    us_train = temp\n",
    "    return np.stack(us_train), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitSeconds(n, country, t):\n",
    "    data = pickle.load( open( \"Raw Track Data\\\\\" + country + \"_\" + t + \".p\", \"rb\" ) )\n",
    "    tracks = data.track_id.unique()\n",
    "    tracks = np.random.choice(tracks, size=n, replace=True)\n",
    "    samples = []\n",
    "    for track in tracks:\n",
    "        try:\n",
    "            trackFeats = data[data.track_id == track]\n",
    "            FeatsLen =  trackFeats.shape[0]\n",
    "            #ind = np.random.choice(trackFeats.index[:-5], size=1, replace=True)\n",
    "            ind = random.randrange(1, FeatsLen - 15)\n",
    "            example = np.empty((0,24))\n",
    "            while len(example) < w_length:\n",
    "                feats = np.array(trackFeats.iloc[ind].loc[\"p1\":\"t12\"])\n",
    "                reps = int(trackFeats.iloc[ind].loc[\"duration\"] * 10)\n",
    "                if reps > 0:\n",
    "                    hold = np.tile(feats, reps).reshape(reps, -1)\n",
    "                    example = np.append(example, hold, axis = 0)\n",
    "                ind = ind + 1\n",
    "              #  example = np.array(example)\n",
    "                example = example[-w_length:]\n",
    "            samples = samples + [example]\n",
    "        except:\n",
    "            continue\n",
    "    samples = np.array(samples)\n",
    "    return samples, np.repeat(np.array([country]), samples.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSamples(train_n, val_n):\n",
    "    train = pd.DataFrame()\n",
    "    train_labels = pd.DataFrame()\n",
    "    val = pd.DataFrame()\n",
    "    val_labels = pd.DataFrame()\n",
    "    train_x = []\n",
    "    train_labels = []\n",
    "    val_x = []\n",
    "    val_labels = []\n",
    "    for country in countriesOfInterest:\n",
    "        print(\"getting\",country)\n",
    "        x1, y1 = splitSeconds(train_n, country, \"train\")\n",
    "        x2, y2 = splitSeconds(val_n, country, \"val\")\n",
    "        train_x = train_x + x1.tolist()\n",
    "        train_labels = train_labels + y1.tolist()\n",
    "        val_x = val_x + x2.tolist()\n",
    "        val_labels = val_labels + y2.tolist()\n",
    "    train_x = np.array(train_x)\n",
    "    train_labels = np.array(train_labels)\n",
    "    val_x = np.array(val_x)\n",
    "    val_labels = np.array(val_labels)\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                     np.unique(train_labels),\n",
    "                                                     list(train_labels))\n",
    "    train_labels = enc.transform(np.array(train_labels).reshape(-1,1)).toarray()\n",
    "    val_labels = enc.transform(np.array(val_labels).reshape(-1,1)).toarray()\n",
    "    return train_x, train_labels, val_x, val_labels, class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting ZA\n",
      "getting EG\n",
      "getting TW\n",
      "getting JP\n",
      "getting DK\n",
      "getting FI\n",
      "getting US\n",
      "getting CA\n",
      "getting AU\n",
      "getting NZ\n",
      "getting BE\n",
      "getting CO\n"
     ]
    }
   ],
   "source": [
    "train_x, train_labels, val_x, val_labels, class_weights = getSamples(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['AU', 'BE', 'CA', 'CO', 'DK', 'EG', 'FI', 'JP', 'NZ', 'TW', 'US',\n",
       "        'ZA'], dtype='<U2')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 300, 64)           22784     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 300, 64)           256       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 300, 128)          98816     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300, 128)          512       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12)                3084      \n",
      "=================================================================\n",
      "Total params: 520,716\n",
      "Trainable params: 519,820\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(LSTM(64, \n",
    "               input_shape=(train_x.shape[1], train_x.shape[2]), \n",
    "               return_sequences = True\n",
    "              ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(128, \n",
    "               input_shape=(train_x.shape[1], train_x.shape[2]), \n",
    "               return_sequences = True,\n",
    "               dropout = .5,\n",
    "              go_backwards = True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(256, \n",
    "               input_shape=(train_x.shape[1], train_x.shape[2]),\n",
    "               dropout = .5\n",
    "              ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.5))\n",
    "#model.add(Dense(64, activation = \"relu\"))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(.5))\n",
    "model.add(Dense(len(enc.categories_[0]), activation= \"softmax\"))\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer= adam, metrics=[\"acc\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting ZA\n",
      "getting EG\n",
      "getting TW\n",
      "getting JP\n",
      "getting DK\n",
      "getting FI\n",
      "getting US\n",
      "getting CA\n",
      "getting AU\n",
      "getting NZ\n",
      "getting BE\n",
      "getting CO\n",
      "[4331. 4353. 4288. 4290. 4268. 4311. 4325. 4373. 4326. 4404. 4308. 4406.]\n",
      "Train on 51983 samples, validate on 10432 samples\n",
      "Epoch 1/25\n",
      "51983/51983 [==============================] - 913s 18ms/sample - loss: 3.2065 - acc: 0.1014 - val_loss: 2.4632 - val_acc: 0.1293\n",
      "Epoch 2/25\n",
      "51983/51983 [==============================] - 740s 14ms/sample - loss: 2.7979 - acc: 0.1139 - val_loss: 2.4470 - val_acc: 0.1393\n",
      "Epoch 3/25\n",
      "51983/51983 [==============================] - 700s 13ms/sample - loss: 2.6158 - acc: 0.1233 - val_loss: 2.4284 - val_acc: 0.1427\n",
      "Epoch 4/25\n",
      "51983/51983 [==============================] - 711s 14ms/sample - loss: 2.5118 - acc: 0.1389 - val_loss: 2.4190 - val_acc: 0.1442\n",
      "Epoch 5/25\n",
      "51983/51983 [==============================] - 709s 14ms/sample - loss: 2.4417 - acc: 0.1579 - val_loss: 2.4075 - val_acc: 0.1595\n",
      "Epoch 6/25\n",
      "51983/51983 [==============================] - 719s 14ms/sample - loss: 2.3940 - acc: 0.1720 - val_loss: 2.4242 - val_acc: 0.1681\n",
      "Epoch 7/25\n",
      "51983/51983 [==============================] - 730s 14ms/sample - loss: 2.3639 - acc: 0.1850 - val_loss: 2.4059 - val_acc: 0.1658\n",
      "Epoch 8/25\n",
      "51983/51983 [==============================] - 748s 14ms/sample - loss: 2.3491 - acc: 0.1907 - val_loss: 2.4265 - val_acc: 0.1732\n",
      "Epoch 9/25\n",
      "51983/51983 [==============================] - 761s 15ms/sample - loss: 2.3311 - acc: 0.1995 - val_loss: 2.3669 - val_acc: 0.1844\n",
      "Epoch 10/25\n",
      "51983/51983 [==============================] - 763s 15ms/sample - loss: 2.3344 - acc: 0.1996 - val_loss: 2.3701 - val_acc: 0.1846\n",
      "Epoch 11/25\n",
      "51983/51983 [==============================] - 765s 15ms/sample - loss: 2.3130 - acc: 0.2058 - val_loss: 2.3723 - val_acc: 0.1820\n",
      "Epoch 12/25\n",
      "51983/51983 [==============================] - 778s 15ms/sample - loss: 2.3064 - acc: 0.2094 - val_loss: 2.4468 - val_acc: 0.1696\n",
      "Epoch 13/25\n",
      "51983/51983 [==============================] - 795s 15ms/sample - loss: 2.3208 - acc: 0.2045 - val_loss: 2.3628 - val_acc: 0.1888\n",
      "Epoch 14/25\n",
      "51983/51983 [==============================] - 791s 15ms/sample - loss: 2.2906 - acc: 0.2150 - val_loss: 2.3571 - val_acc: 0.1918\n",
      "Epoch 15/25\n",
      "51983/51983 [==============================] - 809s 16ms/sample - loss: 2.2803 - acc: 0.2191 - val_loss: 2.3629 - val_acc: 0.1896\n",
      "Epoch 16/25\n",
      "51983/51983 [==============================] - 792s 15ms/sample - loss: 2.2789 - acc: 0.2225 - val_loss: 2.3612 - val_acc: 0.1922\n",
      "Epoch 17/25\n",
      "51983/51983 [==============================] - 790s 15ms/sample - loss: 2.2617 - acc: 0.2253 - val_loss: 2.3525 - val_acc: 0.1962\n",
      "Epoch 18/25\n",
      "51983/51983 [==============================] - 796s 15ms/sample - loss: 2.2514 - acc: 0.2307 - val_loss: 2.3084 - val_acc: 0.2053\n",
      "Epoch 19/25\n",
      "51983/51983 [==============================] - 786s 15ms/sample - loss: 2.2259 - acc: 0.2404 - val_loss: 2.2981 - val_acc: 0.2161\n",
      "Epoch 20/25\n",
      "51983/51983 [==============================] - 807s 16ms/sample - loss: 2.2212 - acc: 0.2423 - val_loss: 2.3276 - val_acc: 0.2040\n",
      "Epoch 21/25\n",
      "51983/51983 [==============================] - 801s 15ms/sample - loss: 2.2387 - acc: 0.2339 - val_loss: 2.2921 - val_acc: 0.2079\n",
      "Epoch 22/25\n",
      "51983/51983 [==============================] - 819s 16ms/sample - loss: 2.2155 - acc: 0.2453 - val_loss: 2.3094 - val_acc: 0.2188\n",
      "Epoch 23/25\n",
      "51983/51983 [==============================] - 789s 15ms/sample - loss: 2.2097 - acc: 0.2455 - val_loss: 2.2672 - val_acc: 0.2243\n",
      "Epoch 24/25\n",
      "51983/51983 [==============================] - 785s 15ms/sample - loss: 2.1879 - acc: 0.2527 - val_loss: 2.2978 - val_acc: 0.2181\n",
      "Epoch 25/25\n",
      "51983/51983 [==============================] - 796s 15ms/sample - loss: 2.1679 - acc: 0.2623 - val_loss: 2.3153 - val_acc: 0.2229\n",
      "10432/10432 [==============================] - 33s 3ms/sample\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51983/51983 [==============================] - 157s 3ms/sample\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting ZA\n",
      "getting EG\n",
      "getting TW\n",
      "getting JP\n",
      "getting DK\n",
      "getting FI\n",
      "getting US\n",
      "getting CA\n",
      "getting AU\n",
      "getting NZ\n",
      "getting BE\n",
      "getting CO\n",
      "[4306. 4355. 4289. 4280. 4287. 4308. 4306. 4401. 4287. 4427. 4275. 4389.]\n",
      "Train on 51910 samples, validate on 10389 samples\n",
      "Epoch 26/50\n",
      "51910/51910 [==============================] - 849s 16ms/sample - loss: 2.2064 - acc: 0.2507 - val_loss: 2.3234 - val_acc: 0.2171\n",
      "Epoch 27/50\n",
      "51910/51910 [==============================] - 970s 19ms/sample - loss: 2.1783 - acc: 0.2588 - val_loss: 2.2999 - val_acc: 0.2269\n",
      "Epoch 28/50\n",
      "51910/51910 [==============================] - 937s 18ms/sample - loss: 2.1560 - acc: 0.2662 - val_loss: 2.3028 - val_acc: 0.2279\n",
      "Epoch 29/50\n",
      "51910/51910 [==============================] - 920s 18ms/sample - loss: 2.1446 - acc: 0.2715 - val_loss: 2.2843 - val_acc: 0.2343\n",
      "Epoch 30/50\n",
      "51910/51910 [==============================] - 902s 17ms/sample - loss: 2.1304 - acc: 0.2764 - val_loss: 2.2905 - val_acc: 0.2312\n",
      "Epoch 31/50\n",
      "51910/51910 [==============================] - 898s 17ms/sample - loss: 2.1164 - acc: 0.2832 - val_loss: 2.2913 - val_acc: 0.2329\n",
      "Epoch 32/50\n",
      "51910/51910 [==============================] - 899s 17ms/sample - loss: 2.1015 - acc: 0.2886 - val_loss: 2.2755 - val_acc: 0.2345\n",
      "Epoch 33/50\n",
      "51910/51910 [==============================] - 910s 18ms/sample - loss: 2.0901 - acc: 0.2918 - val_loss: 2.2863 - val_acc: 0.2386\n",
      "Epoch 34/50\n",
      "51910/51910 [==============================] - 918s 18ms/sample - loss: 2.0815 - acc: 0.2971 - val_loss: 2.2999 - val_acc: 0.2386\n",
      "Epoch 35/50\n",
      "51910/51910 [==============================] - 899s 17ms/sample - loss: 2.0682 - acc: 0.2999 - val_loss: 2.3170 - val_acc: 0.2378\n",
      "Epoch 36/50\n",
      "51910/51910 [==============================] - 910s 18ms/sample - loss: 2.0562 - acc: 0.3070 - val_loss: 2.3019 - val_acc: 0.2374\n",
      "Epoch 37/50\n",
      "51910/51910 [==============================] - 903s 17ms/sample - loss: 2.0412 - acc: 0.3114 - val_loss: 2.3060 - val_acc: 0.2404\n",
      "Epoch 38/50\n",
      "51910/51910 [==============================] - 897s 17ms/sample - loss: 2.0316 - acc: 0.3148 - val_loss: 2.3273 - val_acc: 0.2425\n",
      "Epoch 39/50\n",
      "51910/51910 [==============================] - 927s 18ms/sample - loss: 2.0191 - acc: 0.3202 - val_loss: 2.3369 - val_acc: 0.2443\n",
      "Epoch 40/50\n",
      "51910/51910 [==============================] - 935s 18ms/sample - loss: 2.0025 - acc: 0.3247 - val_loss: 2.3354 - val_acc: 0.2415\n",
      "Epoch 41/50\n",
      "51910/51910 [==============================] - 953s 18ms/sample - loss: 1.9889 - acc: 0.3314 - val_loss: 2.3314 - val_acc: 0.2395\n",
      "Epoch 42/50\n",
      "51910/51910 [==============================] - 931s 18ms/sample - loss: 1.9745 - acc: 0.3365 - val_loss: 2.3561 - val_acc: 0.2387\n",
      "Epoch 43/50\n",
      "51910/51910 [==============================] - 926s 18ms/sample - loss: 1.9595 - acc: 0.3400 - val_loss: 2.3814 - val_acc: 0.2351\n",
      "Epoch 44/50\n",
      "51910/51910 [==============================] - 936s 18ms/sample - loss: 1.9430 - acc: 0.3470 - val_loss: 2.4176 - val_acc: 0.2368\n",
      "Epoch 45/50\n",
      "51910/51910 [==============================] - 940s 18ms/sample - loss: 1.9311 - acc: 0.3524 - val_loss: 2.4183 - val_acc: 0.2434\n",
      "Epoch 46/50\n",
      "46080/51910 [=========================>....] - ETA: 1:45 - loss: 1.9134 - acc: 0.3609"
     ]
    }
   ],
   "source": [
    "log_dir = os.path.join(\n",
    "    \"logs\",\n",
    "    \"fit\",\n",
    "    \"64_128_256_Dropout_5000sample\"\n",
    ")\n",
    "train_n = 5000\n",
    "val_n = 1000\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "epochs = 25\n",
    "iterations = 10\n",
    "learn_rate = 0.001\n",
    "for i in range(iterations):\n",
    "    adam = keras.optimizers.Adam(lr=learn_rate)\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer= adam, metrics=[\"acc\"])\n",
    "    train_x, train_labels, val_x, val_labels, class_weights = getSamples(train_n, val_n)\n",
    "    print(np.sum(train_labels, axis = 0))\n",
    "    model.fit(train_x, train_labels,\n",
    "              epochs = i * epochs + epochs, \n",
    "              initial_epoch = i * epochs,\n",
    "              shuffle = True,\n",
    "              validation_data = (val_x, val_labels),\n",
    "              batch_size = 1024,\n",
    "              class_weight = class_weights,\n",
    "             callbacks=[tensorboard_callback],\n",
    "             verbose = 1)\n",
    "    learn_rate = learn_rate/2\n",
    "    if i % 1 == 0:\n",
    "        preds = model.predict(val_x, batch_size = 1024, verbose = 1)\n",
    "     #   print(np.sum(train_labels, axis = 0))\n",
    "        plt.imshow(\n",
    "            confusion_matrix(\n",
    "                enc.inverse_transform(preds), \n",
    "                enc.inverse_transform(val_labels), \n",
    "               # normalize = \"all\"\n",
    "            )\n",
    "        )\n",
    "        plt.pause(.5)\n",
    "        plt.show()\n",
    "        preds = model.predict(train_x, batch_size = 1024, verbose = 1)\n",
    "        plt.imshow(\n",
    "            confusion_matrix(\n",
    "                enc.inverse_transform(preds), \n",
    "                enc.inverse_transform(train_labels), \n",
    "            #    normalize = \"all\"\n",
    "            )\n",
    "        )\n",
    "        plt.pause(.5)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
