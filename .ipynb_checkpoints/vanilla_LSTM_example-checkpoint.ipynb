{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234) ### DOES NOT WORK???\n",
    "w_length = 100\n",
    "n_countries = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = pd.DataFrame()\n",
    "us = pd.DataFrame()\n",
    "for file in glob.glob(\"Raw Track Data\\\\*.csv\"):\n",
    "    name = file[15:-4]\n",
    "    new = pd.read_csv(file)\n",
    "    if name[:2] == \"AD\":\n",
    "        mx = mx.append(new)\n",
    "    if name[:2] == \"DZ\":\n",
    "        us = us.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = mx.drop([\"confidence\", \"loudness_start\", \"loudness_max_time\"], axis = 1)\n",
    "us = us.drop([\"confidence\", \"loudness_start\", \"loudness_max_time\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140769, 28)\n",
      "(140769, 28)\n"
     ]
    }
   ],
   "source": [
    "print(mx.shape)\n",
    "mx = mx.drop_duplicates([\"track_id\", \"start\"])\n",
    "print(mx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(259215, 28)\n",
      "(224323, 28)\n"
     ]
    }
   ],
   "source": [
    "print(us.shape)\n",
    "us = us.drop_duplicates([\"track_id\", \"start\"])\n",
    "print(us.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MX (140769, 28)\n",
      "US (224323, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"MX\", mx.shape)\n",
    "print(\"US\", us.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the playlists have common songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(us.track_id.isin(mx.track_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MX (140769, 28)\n",
      "US (224323, 28)\n"
     ]
    }
   ],
   "source": [
    "dupes = us[\"track_id\"].loc[us.track_id.isin(mx.track_id)]\n",
    "mx = mx.loc[~(mx[\"track_id\"].isin(dupes))]\n",
    "us = us.loc[~(us[\"track_id\"].isin(dupes))]\n",
    "print(\"MX\", mx.shape)\n",
    "print(\"US\", us.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input array for US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pos = list(us.track_id.index[us.track_id.shift(1) != us.track_id]) # indices where the song changes\n",
    "new_pos.append(max(us.track_id.index) + 1) # add a new index to know where the last song ends\n",
    "split_pos = []\n",
    "for i in range(len(new_pos)-1):\n",
    "    split_pos = split_pos + list(range(new_pos[i], new_pos[i+1], w_length))\n",
    "split_pos = split_pos[1:]\n",
    "us_train = np.split(us.iloc[:,:28].to_numpy(), split_pos)\n",
    "# drop the short sequences\n",
    "short_seqs = []\n",
    "temp = [] \n",
    "for i, value in enumerate(us_train):\n",
    "    if value.shape[0] == w_length:\n",
    "        temp.append(value)\n",
    "us_train = temp\n",
    "us_train = np.stack(us_train)\n",
    "val_index = np.isin(us_train[:,:,27],\n",
    "                    np.random.choice(us.track_id.unique(), np.int(len(us.track_id.unique())/10)))\n",
    "val_index = val_index.sum(1) != 0\n",
    "us_val = us_train[val_index,:,:26] # drop track id\n",
    "us_train = us_train[np.logical_not(val_index),:,:26] # drop track id\n",
    "us_train = us_train.astype(\"float64\")\n",
    "us_val = us_val.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want songs to have the same number of observations, we can determine which element in the list returned by np.slit corresponds to which song (using new_pos) and randomly overpopulate accordingly. An alternative is to do this with an online batching if we have memory problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input array for MX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pos = list(mx.track_id.index[mx.track_id.shift(1) != mx.track_id]) # indices where the song changes\n",
    "new_pos.append(max(mx.track_id.index) + 1) # add a new index to know where the last song ends\n",
    "split_pos = []\n",
    "for i in range(len(new_pos)-1):\n",
    "    split_pos = split_pos + list(range(new_pos[i], new_pos[i+1], w_length))\n",
    "split_pos = split_pos[1:]\n",
    "mx_train = np.split(mx.iloc[:,:28].to_numpy(), split_pos)\n",
    "# drop the short sequences\n",
    "short_seqs = []\n",
    "temp = [] \n",
    "for i, value in enumerate(mx_train):\n",
    "    if value.shape[0] == w_length:\n",
    "        temp.append(value)\n",
    "mx_train = temp\n",
    "mx_train = np.stack(mx_train)\n",
    "val_index = np.isin(mx_train[:,:,27],\n",
    "                    np.random.choice(mx.track_id.unique(), np.int(len(mx.track_id.unique())/10)))\n",
    "val_index = val_index.sum(1) != 0\n",
    "mx_val = mx_train[val_index,:,:26] # drop track id\n",
    "mx_train = mx_train[np.logical_not(val_index),:,:26] # drop track id\n",
    "mx_train = mx_train.astype(\"float64\")\n",
    "mx_val = mx_val.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Output arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3917, 100, 26) \n",
      " (397, 100, 26) \n",
      " (3917, 2) \n",
      " (397, 2)\n",
      "float64 \n",
      " float64 \n",
      " float32 \n",
      " float32\n"
     ]
    }
   ],
   "source": [
    "train_input = np.concatenate([us_train, mx_train])\n",
    "val_input = np.concatenate([us_val, mx_val])\n",
    "\n",
    "train_output = np.ones((us_train.shape[0], 1))\n",
    "train_output = np.concatenate([train_output, np.zeros((mx_train.shape[0], 1))])\n",
    "train_output = keras.utils.to_categorical(train_output)\n",
    "val_output = np.ones((us_val.shape[0], 1))\n",
    "val_output = np.concatenate([val_output, np.zeros((mx_val.shape[0], 1))])\n",
    "val_output = keras.utils.to_categorical(val_output)\n",
    "\n",
    "\n",
    "print(train_input.shape, \"\\n\", val_input.shape, \"\\n\", train_output.shape, \"\\n\", val_output.shape)\n",
    "print(train_input.dtype, \"\\n\", val_input.dtype, \"\\n\", train_output.dtype, \"\\n\", val_output.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 25)                5200      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 52        \n",
      "=================================================================\n",
      "Total params: 5,252\n",
      "Trainable params: 5,252\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "out_index = 2\n",
    "model = keras.Sequential()\n",
    "model.add(LSTM(25, input_shape=(w_length, train_input.shape[2]), dropout = .5, recurrent_dropout = .5))\n",
    "model.add(Dense(n_countries, activation= \"softmax\"))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer= \"adam\", metrics=[\"acc\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_input)\n",
    "train_input.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3917 samples, validate on 397 samples\n",
      "Epoch 1/50\n",
      "3917/3917 [==============================] - 3s 745us/sample - loss: 0.8168 - acc: 0.4542 - val_loss: 0.7626 - val_acc: 0.4861\n",
      "Epoch 2/50\n",
      "3917/3917 [==============================] - 1s 377us/sample - loss: 0.7288 - acc: 0.5198 - val_loss: 0.6976 - val_acc: 0.5038\n",
      "Epoch 3/50\n",
      "3917/3917 [==============================] - 1s 366us/sample - loss: 0.7041 - acc: 0.5484 - val_loss: 0.6676 - val_acc: 0.6322\n",
      "Epoch 4/50\n",
      "3917/3917 [==============================] - 1s 368us/sample - loss: 0.6785 - acc: 0.5788 - val_loss: 0.6530 - val_acc: 0.6448\n",
      "Epoch 5/50\n",
      "3917/3917 [==============================] - 1s 373us/sample - loss: 0.6715 - acc: 0.5977 - val_loss: 0.6444 - val_acc: 0.6322\n",
      "Epoch 6/50\n",
      "3917/3917 [==============================] - 1s 378us/sample - loss: 0.6677 - acc: 0.6035 - val_loss: 0.6374 - val_acc: 0.6448\n",
      "Epoch 7/50\n",
      "3917/3917 [==============================] - 1s 368us/sample - loss: 0.6661 - acc: 0.6017 - val_loss: 0.6265 - val_acc: 0.6549\n",
      "Epoch 8/50\n",
      "3917/3917 [==============================] - 1s 370us/sample - loss: 0.6535 - acc: 0.6260 - val_loss: 0.6243 - val_acc: 0.6625\n",
      "Epoch 9/50\n",
      "3917/3917 [==============================] - 1s 383us/sample - loss: 0.6506 - acc: 0.6242 - val_loss: 0.6227 - val_acc: 0.6599\n",
      "Epoch 10/50\n",
      "3917/3917 [==============================] - 2s 390us/sample - loss: 0.6584 - acc: 0.6145 - val_loss: 0.6207 - val_acc: 0.6423\n",
      "Epoch 11/50\n",
      "3917/3917 [==============================] - 1s 352us/sample - loss: 0.6568 - acc: 0.6199 - val_loss: 0.6244 - val_acc: 0.6247\n",
      "Epoch 12/50\n",
      "3917/3917 [==============================] - 1s 371us/sample - loss: 0.6472 - acc: 0.6265 - val_loss: 0.6152 - val_acc: 0.6348\n",
      "Epoch 13/50\n",
      "3917/3917 [==============================] - 1s 381us/sample - loss: 0.6469 - acc: 0.6298 - val_loss: 0.6060 - val_acc: 0.6499\n",
      "Epoch 14/50\n",
      "3917/3917 [==============================] - 1s 377us/sample - loss: 0.6492 - acc: 0.6247 - val_loss: 0.6026 - val_acc: 0.6524\n",
      "Epoch 15/50\n",
      "3917/3917 [==============================] - 1s 357us/sample - loss: 0.6416 - acc: 0.6342 - val_loss: 0.5929 - val_acc: 0.6650\n",
      "Epoch 16/50\n",
      "3917/3917 [==============================] - 1s 355us/sample - loss: 0.6376 - acc: 0.6372 - val_loss: 0.5867 - val_acc: 0.6751\n",
      "Epoch 17/50\n",
      "3917/3917 [==============================] - 1s 352us/sample - loss: 0.6406 - acc: 0.6388 - val_loss: 0.5908 - val_acc: 0.6574\n",
      "Epoch 18/50\n",
      "3917/3917 [==============================] - 1s 364us/sample - loss: 0.6321 - acc: 0.6513 - val_loss: 0.5943 - val_acc: 0.6373\n",
      "Epoch 19/50\n",
      "3917/3917 [==============================] - 1s 375us/sample - loss: 0.6381 - acc: 0.6326 - val_loss: 0.5825 - val_acc: 0.6700\n",
      "Epoch 20/50\n",
      "3917/3917 [==============================] - 1s 357us/sample - loss: 0.6350 - acc: 0.6319 - val_loss: 0.5796 - val_acc: 0.6826\n",
      "Epoch 21/50\n",
      "3917/3917 [==============================] - 1s 361us/sample - loss: 0.6333 - acc: 0.6421 - val_loss: 0.5711 - val_acc: 0.7003\n",
      "Epoch 22/50\n",
      "3917/3917 [==============================] - 1s 349us/sample - loss: 0.6325 - acc: 0.6454 - val_loss: 0.5635 - val_acc: 0.7229\n",
      "Epoch 23/50\n",
      "3917/3917 [==============================] - 1s 359us/sample - loss: 0.6320 - acc: 0.6444 - val_loss: 0.5748 - val_acc: 0.6801\n",
      "Epoch 24/50\n",
      "3917/3917 [==============================] - 1s 362us/sample - loss: 0.6321 - acc: 0.6446 - val_loss: 0.5839 - val_acc: 0.6499\n",
      "Epoch 25/50\n",
      "3917/3917 [==============================] - 1s 353us/sample - loss: 0.6241 - acc: 0.6574 - val_loss: 0.5819 - val_acc: 0.6474\n",
      "Epoch 26/50\n",
      "3917/3917 [==============================] - 1s 350us/sample - loss: 0.6296 - acc: 0.6508 - val_loss: 0.5757 - val_acc: 0.6524\n",
      "Epoch 27/50\n",
      "3917/3917 [==============================] - 1s 356us/sample - loss: 0.6237 - acc: 0.6610 - val_loss: 0.5709 - val_acc: 0.6574\n",
      "Epoch 28/50\n",
      "3917/3917 [==============================] - 1s 351us/sample - loss: 0.6311 - acc: 0.6467 - val_loss: 0.5619 - val_acc: 0.6625\n",
      "Epoch 29/50\n",
      "3917/3917 [==============================] - 1s 353us/sample - loss: 0.6239 - acc: 0.6617 - val_loss: 0.5635 - val_acc: 0.6725\n",
      "Epoch 30/50\n",
      "3917/3917 [==============================] - 1s 343us/sample - loss: 0.6226 - acc: 0.6569 - val_loss: 0.5573 - val_acc: 0.6776\n",
      "Epoch 31/50\n",
      "3917/3917 [==============================] - 2s 395us/sample - loss: 0.6222 - acc: 0.6589 - val_loss: 0.5535 - val_acc: 0.6801\n",
      "Epoch 32/50\n",
      "3917/3917 [==============================] - 1s 369us/sample - loss: 0.6280 - acc: 0.6533 - val_loss: 0.5526 - val_acc: 0.6751\n",
      "Epoch 33/50\n",
      "3917/3917 [==============================] - 1s 365us/sample - loss: 0.6208 - acc: 0.6638 - val_loss: 0.5491 - val_acc: 0.6877\n",
      "Epoch 34/50\n",
      "3917/3917 [==============================] - 1s 349us/sample - loss: 0.6246 - acc: 0.6594 - val_loss: 0.5539 - val_acc: 0.6725\n",
      "Epoch 35/50\n",
      "3917/3917 [==============================] - 1s 344us/sample - loss: 0.6191 - acc: 0.6597 - val_loss: 0.5598 - val_acc: 0.6574\n",
      "Epoch 36/50\n",
      "3917/3917 [==============================] - 1s 347us/sample - loss: 0.6156 - acc: 0.6587 - val_loss: 0.5552 - val_acc: 0.6700\n",
      "Epoch 37/50\n",
      "3917/3917 [==============================] - 1s 339us/sample - loss: 0.6222 - acc: 0.6599 - val_loss: 0.5595 - val_acc: 0.6801\n",
      "Epoch 38/50\n",
      "3917/3917 [==============================] - 1s 346us/sample - loss: 0.6082 - acc: 0.6709 - val_loss: 0.5503 - val_acc: 0.6877\n",
      "Epoch 39/50\n",
      "3917/3917 [==============================] - 1s 359us/sample - loss: 0.6102 - acc: 0.6702 - val_loss: 0.5485 - val_acc: 0.6952\n",
      "Epoch 40/50\n",
      "3917/3917 [==============================] - 1s 364us/sample - loss: 0.6123 - acc: 0.6768 - val_loss: 0.5544 - val_acc: 0.6826\n",
      "Epoch 41/50\n",
      "3917/3917 [==============================] - 2s 388us/sample - loss: 0.6084 - acc: 0.6714 - val_loss: 0.5445 - val_acc: 0.6927\n",
      "Epoch 42/50\n",
      "3917/3917 [==============================] - 2s 396us/sample - loss: 0.6137 - acc: 0.6714 - val_loss: 0.5403 - val_acc: 0.7028\n",
      "Epoch 43/50\n",
      "3917/3917 [==============================] - 2s 402us/sample - loss: 0.6133 - acc: 0.6648 - val_loss: 0.5394 - val_acc: 0.7204\n",
      "Epoch 44/50\n",
      "3917/3917 [==============================] - 1s 353us/sample - loss: 0.6109 - acc: 0.6707 - val_loss: 0.5354 - val_acc: 0.7103\n",
      "Epoch 45/50\n",
      "3917/3917 [==============================] - 1s 365us/sample - loss: 0.6018 - acc: 0.6786 - val_loss: 0.5395 - val_acc: 0.7128\n",
      "Epoch 46/50\n",
      "3917/3917 [==============================] - 1s 361us/sample - loss: 0.6111 - acc: 0.6694 - val_loss: 0.5276 - val_acc: 0.7229\n",
      "Epoch 47/50\n",
      "3917/3917 [==============================] - 1s 346us/sample - loss: 0.6113 - acc: 0.6801 - val_loss: 0.5301 - val_acc: 0.7254\n",
      "Epoch 48/50\n",
      "3917/3917 [==============================] - 1s 351us/sample - loss: 0.6091 - acc: 0.6753 - val_loss: 0.5233 - val_acc: 0.7431\n",
      "Epoch 49/50\n",
      "3917/3917 [==============================] - 1s 347us/sample - loss: 0.6090 - acc: 0.6765 - val_loss: 0.5029 - val_acc: 0.7506\n",
      "Epoch 50/50\n",
      "3917/3917 [==============================] - 1s 371us/sample - loss: 0.6114 - acc: 0.6612 - val_loss: 0.5016 - val_acc: 0.7582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2398380d588>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_input, train_output,\n",
    "          epochs = 50, shuffle = True,\n",
    "          validation_data = (val_input, val_output),\n",
    "          batch_size = 128,\n",
    "         use_multiprocessing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
