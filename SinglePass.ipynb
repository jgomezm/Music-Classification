{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, LSTM, Flatten, BatchNormalization, Dropout\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.13490121, 1.14730664, 2.29595517, 0.76382944, 1.31395025,\n",
       "       1.07741035, 1.34639346, 2.89534169, 0.57541893, 1.00927592,\n",
       "       0.72144126, 0.49428218])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = pickle.load( open( \"pickle\\\\enc.p\", \"rb\" ) )\n",
    "val_x = pickle.load( open( \"pickle\\\\val_x.p\", \"rb\" ) )\n",
    "train_x = pickle.load( open( \"pickle\\\\train_x.p\", \"rb\" ) )\n",
    "val_labels = pickle.load( open( \"pickle\\\\val_labels.p\", \"rb\" ) )\n",
    "train_labels = pickle.load( open( \"pickle\\\\train_labels.p\", \"rb\" ) )\n",
    "class_weights = pickle.load( open( \"pickle\\\\class_weights.p\", \"rb\" ) )\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(\n",
    "    \"Singlelogs\",\n",
    "    \"fit\",\n",
    "    \"1632\",\n",
    "    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    ")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16050966307508124"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(np.sum(val_labels, axis = 0))/sum(np.sum(val_labels, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 200, 16)           2816      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200)               6600      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                2412      \n",
      "=================================================================\n",
      "Total params: 18,100\n",
      "Trainable params: 18,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(LSTM(16, \n",
    "               input_shape=(train_x.shape[1], train_x.shape[2]), \n",
    "               return_sequences = True))\n",
    "model.add(LSTM(32, \n",
    "               input_shape=(train_x.shape[1], train_x.shape[2]), \n",
    "               return_sequences = False))\n",
    "model.add(Dense(200, activation= \"softmax\"))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(.25))\n",
    "model.add(Dense(len(enc.categories_[0]), activation= \"softmax\"))\n",
    "adam = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer= adam, metrics=[\"acc\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47113 samples, validate on 11694 samples\n",
      "Epoch 1/50\n",
      "47113/47113 [==============================] - 36s 767us/sample - loss: 2.4849 - acc: 0.0789 - val_loss: 2.4838 - val_acc: 0.1117\n",
      "Epoch 2/50\n",
      "47113/47113 [==============================] - 38s 814us/sample - loss: 2.4829 - acc: 0.1155 - val_loss: 2.4819 - val_acc: 0.1117\n",
      "Epoch 3/50\n",
      "47113/47113 [==============================] - 43s 909us/sample - loss: 2.4809 - acc: 0.1155 - val_loss: 2.4800 - val_acc: 0.1117\n",
      "Epoch 4/50\n",
      "47113/47113 [==============================] - 43s 913us/sample - loss: 2.4789 - acc: 0.1155 - val_loss: 2.4780 - val_acc: 0.1117\n",
      "Epoch 5/50\n",
      "47113/47113 [==============================] - 44s 933us/sample - loss: 2.4768 - acc: 0.1155 - val_loss: 2.4759 - val_acc: 0.1117\n",
      "Epoch 6/50\n",
      "47113/47113 [==============================] - 44s 927us/sample - loss: 2.4745 - acc: 0.1155 - val_loss: 2.4736 - val_acc: 0.1117\n",
      "Epoch 7/50\n",
      "47113/47113 [==============================] - 44s 925us/sample - loss: 2.4719 - acc: 0.1155 - val_loss: 2.4710 - val_acc: 0.1117\n",
      "Epoch 8/50\n",
      "47113/47113 [==============================] - 43s 911us/sample - loss: 2.4692 - acc: 0.1156 - val_loss: 2.4683 - val_acc: 0.1601\n",
      "Epoch 9/50\n",
      "47113/47113 [==============================] - 43s 912us/sample - loss: 2.4663 - acc: 0.1685 - val_loss: 2.4656 - val_acc: 0.1605\n",
      "Epoch 10/50\n",
      "47113/47113 [==============================] - 44s 937us/sample - loss: 2.4635 - acc: 0.1686 - val_loss: 2.4628 - val_acc: 0.1605\n",
      "Epoch 11/50\n",
      "47113/47113 [==============================] - 44s 926us/sample - loss: 2.4606 - acc: 0.1686 - val_loss: 2.4601 - val_acc: 0.1605\n",
      "Epoch 12/50\n",
      "47113/47113 [==============================] - 45s 958us/sample - loss: 2.4578 - acc: 0.1686 - val_loss: 2.4574 - val_acc: 0.1605\n",
      "Epoch 13/50\n",
      "47113/47113 [==============================] - 45s 962us/sample - loss: 2.4551 - acc: 0.1686 - val_loss: 2.4547 - val_acc: 0.1605\n",
      "Epoch 14/50\n",
      "47113/47113 [==============================] - 46s 983us/sample - loss: 2.4523 - acc: 0.1686 - val_loss: 2.4522 - val_acc: 0.1605\n",
      "Epoch 15/50\n",
      "47113/47113 [==============================] - 47s 990us/sample - loss: 2.4497 - acc: 0.1686 - val_loss: 2.4497 - val_acc: 0.1605\n",
      "Epoch 16/50\n",
      "47113/47113 [==============================] - 47s 1000us/sample - loss: 2.4471 - acc: 0.1686 - val_loss: 2.4472 - val_acc: 0.1605\n",
      "Epoch 17/50\n",
      "47113/47113 [==============================] - 48s 1ms/sample - loss: 2.4446 - acc: 0.1686 - val_loss: 2.4448 - val_acc: 0.1605\n",
      "Epoch 18/50\n",
      "47113/47113 [==============================] - 48s 1ms/sample - loss: 2.4422 - acc: 0.1686 - val_loss: 2.4427 - val_acc: 0.1605\n",
      "Epoch 19/50\n",
      "47113/47113 [==============================] - 48s 1ms/sample - loss: 2.4400 - acc: 0.1686 - val_loss: 2.4406 - val_acc: 0.1605\n",
      "Epoch 20/50\n",
      "47113/47113 [==============================] - 48s 1ms/sample - loss: 2.4378 - acc: 0.1686 - val_loss: 2.4386 - val_acc: 0.1605\n",
      "Epoch 21/50\n",
      "47113/47113 [==============================] - 48s 1ms/sample - loss: 2.4357 - acc: 0.1686 - val_loss: 2.4365 - val_acc: 0.1605\n",
      "Epoch 22/50\n",
      "47113/47113 [==============================] - 48s 1ms/sample - loss: 2.4336 - acc: 0.1686 - val_loss: 2.4345 - val_acc: 0.1605\n",
      "Epoch 23/50\n",
      "47113/47113 [==============================] - 49s 1ms/sample - loss: 2.4315 - acc: 0.1686 - val_loss: 2.4326 - val_acc: 0.1605\n",
      "Epoch 24/50\n",
      "47113/47113 [==============================] - 49s 1ms/sample - loss: 2.4295 - acc: 0.1686 - val_loss: 2.4306 - val_acc: 0.1605\n",
      "Epoch 25/50\n",
      "47113/47113 [==============================] - 49s 1ms/sample - loss: 2.4275 - acc: 0.1686 - val_loss: 2.4289 - val_acc: 0.1605\n",
      "Epoch 26/50\n",
      "47113/47113 [==============================] - 51s 1ms/sample - loss: 2.4257 - acc: 0.1686 - val_loss: 2.4270 - val_acc: 0.1605\n",
      "Epoch 27/50\n",
      "47113/47113 [==============================] - 51s 1ms/sample - loss: 2.4238 - acc: 0.1686 - val_loss: 2.4253 - val_acc: 0.1605\n",
      "Epoch 28/50\n",
      "47113/47113 [==============================] - 51s 1ms/sample - loss: 2.4220 - acc: 0.1686 - val_loss: 2.4236 - val_acc: 0.1605\n",
      "Epoch 29/50\n",
      "47113/47113 [==============================] - 51s 1ms/sample - loss: 2.4203 - acc: 0.1686 - val_loss: 2.4220 - val_acc: 0.1605\n",
      "Epoch 30/50\n",
      "47113/47113 [==============================] - 50s 1ms/sample - loss: 2.4187 - acc: 0.1686 - val_loss: 2.4204 - val_acc: 0.1605\n",
      "Epoch 31/50\n",
      "47113/47113 [==============================] - 51s 1ms/sample - loss: 2.4169 - acc: 0.1686 - val_loss: 2.4188 - val_acc: 0.1605\n",
      "Epoch 32/50\n",
      "47113/47113 [==============================] - 52s 1ms/sample - loss: 2.4153 - acc: 0.1686 - val_loss: 2.4172 - val_acc: 0.1605\n",
      "Epoch 33/50\n",
      "47113/47113 [==============================] - 51s 1ms/sample - loss: 2.4137 - acc: 0.1686 - val_loss: 2.4158 - val_acc: 0.1605\n",
      "Epoch 34/50\n",
      "47113/47113 [==============================] - 52s 1ms/sample - loss: 2.4121 - acc: 0.1686 - val_loss: 2.4143 - val_acc: 0.1605\n",
      "Epoch 35/50\n",
      "47113/47113 [==============================] - 52s 1ms/sample - loss: 2.4106 - acc: 0.1686 - val_loss: 2.4128 - val_acc: 0.1605\n",
      "Epoch 36/50\n",
      "47113/47113 [==============================] - 52s 1ms/sample - loss: 2.4090 - acc: 0.1686 - val_loss: 2.4113 - val_acc: 0.1605\n",
      "Epoch 37/50\n",
      "47113/47113 [==============================] - 52s 1ms/sample - loss: 2.4075 - acc: 0.1686 - val_loss: 2.4099 - val_acc: 0.1605\n",
      "Epoch 38/50\n",
      "47113/47113 [==============================] - 52s 1ms/sample - loss: 2.4060 - acc: 0.1686 - val_loss: 2.4085 - val_acc: 0.1605\n",
      "Epoch 39/50\n",
      "47113/47113 [==============================] - 53s 1ms/sample - loss: 2.4046 - acc: 0.1686 - val_loss: 2.4072 - val_acc: 0.1605\n",
      "Epoch 40/50\n",
      "47113/47113 [==============================] - 53s 1ms/sample - loss: 2.4032 - acc: 0.1686 - val_loss: 2.4059 - val_acc: 0.1605\n",
      "Epoch 41/50\n",
      "47113/47113 [==============================] - 53s 1ms/sample - loss: 2.4018 - acc: 0.1686 - val_loss: 2.4046 - val_acc: 0.1605\n",
      "Epoch 42/50\n",
      "47113/47113 [==============================] - 53s 1ms/sample - loss: 2.4004 - acc: 0.1686 - val_loss: 2.4033 - val_acc: 0.1605\n",
      "Epoch 43/50\n",
      "47113/47113 [==============================] - 66s 1ms/sample - loss: 2.3990 - acc: 0.1686 - val_loss: 2.4019 - val_acc: 0.1605\n",
      "Epoch 44/50\n",
      "30720/47113 [==================>...........] - ETA: 17s - loss: 2.3981 - acc: 0.1680"
     ]
    }
   ],
   "source": [
    "model.fit(train_x, train_labels,\n",
    "          epochs = 50, \n",
    "          shuffle = True,\n",
    "          validation_data = (val_x, val_labels),\n",
    "          batch_size = 2048,\n",
    "         class_weight = class_weights,\n",
    "         callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(train_x, batch_size = 256, verbose = 1)\n",
    "print(np.sum(preds, axis = 0))\n",
    "print(np.sum(train_labels, axis = 0))\n",
    "plt.imshow(\n",
    "    confusion_matrix(\n",
    "        enc.inverse_transform(preds), \n",
    "        enc.inverse_transform(train_labels), \n",
    "        normalize = \"true\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(val_x, batch_size = 256, verbose = 1)\n",
    "print(np.sum(preds, axis = 0))\n",
    "print(np.sum(val_labels, axis = 0))\n",
    "plt.imshow(\n",
    "    confusion_matrix(\n",
    "        enc.inverse_transform(preds), \n",
    "        enc.inverse_transform(val_labels), \n",
    "        normalize = \"true\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(\n",
    "    \"Singlelogs\",\n",
    "    \"fit\",\n",
    "    \"addDense\",\n",
    "    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    ")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_x, train_labels,\n",
    "          epochs = 100, \n",
    "          shuffle = True,\n",
    "          validation_data = (val_x, val_labels),\n",
    "          batch_size = 1024,\n",
    "         class_weight = class_weights,\n",
    "         callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(train_x, batch_size = 256, verbose = 1)\n",
    "print(np.sum(preds, axis = 0))\n",
    "print(np.sum(train_labels, axis = 0))\n",
    "plt.imshow(\n",
    "    confusion_matrix(\n",
    "        enc.inverse_transform(preds), \n",
    "        enc.inverse_transform(train_labels), \n",
    "        normalize = \"true\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(val_x, batch_size = 256, verbose = 1)\n",
    "print(np.sum(preds, axis = 0))\n",
    "print(np.sum(val_labels, axis = 0))\n",
    "plt.imshow(\n",
    "    confusion_matrix(\n",
    "        enc.inverse_transform(preds), \n",
    "        enc.inverse_transform(val_labels), \n",
    "        normalize = \"true\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
